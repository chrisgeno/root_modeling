# Data Warehouse Work Sample Solution Chris Geno (chris@chrisgeno.net)

## <a id="toc-table-of-contents"></a> Table of Contents:
------
- [Introduction](#toc-introduction)
- [Assumptions](#toc-assumptions)
- [Initial Basic Measures We Want to Track and their schema](#toc-initial-solution)
- [A Step Further](#toc-a-step-further)
- [ETL and Infrastructure](#toc-etl-infrastructure)
- [Testing and Debugging](#toc-testing-and-debugging)

[^back](#toc-table-of-contents)

## <a id="toc-introduction"></a>Introduction:

When considering a data selection problem with an already perfectly structured 2NF schema as provided, my initial thought is to simply denormalize the data at the highest resolution required to minimize join performance hits and provide a basic reporting table. However, taking into account that "we often want to slice and dice these measurements" and we have a variety of types of analysts working with it with varied levels of query skill and tools, a partially denormalized schema that stands up to more performance scrutiny or "weird" tool generated queries and related processes is what I'll create.

[^back](#toc-table-of-contents)

## <a id="toc-assumptions"></a> Assumptions:

* The data in the existing schema is already well formed. IE: dates aren't at impossible times in the future, people aren't a million years old. Essentially I won't check for outlier values with constraints that would obliterate statistics. 
* Updates to important dimensions are relatively infrequent compared to the amount of analysis and ingestion we're doing, like marital status, birthdate, or license state
* Users will have varying levels of skill and toolsets to access this data. IE: I will attempt to optimize for tools like Tableau auto generating weird queries with suboptimal subqueries.
* These numbers do not need to be updated in real time. Some data staleness is tolerable so a job of some frequency like hourly or nightly keeps our data sufficiently up to date.
* For simplification, I'll also assume we aren't doing analysis on the app's use itself. IE: the relationship between how often or when a particular object is updated is less important than the actual statistics and dimensions generated through driving activities relevant to insurance rates. We're not looking for how often or why users might be updating their profile across various dimensions.

[^back](#toc-table-of-contents)

## <a id="toc-initial-solution"></a> Initial Basic Measures We Want to Track and a schema, (The oversimplified single query solution we don't want):

Even though this is not what we want. Quickly creating a query that provides a basic denormalized table of the measurements asked for is something I like to do to setup a basic structure to think about. So I'll put that here and then build off of it.

``` sql
select d.id as driver_id,
       t.id as trip_id,
       datediff(minute, started_at, ended_at) as trip_duration,
       t.distance,
       t.started_at as trip_start,
       t.ended_at as trip_end,
from driver d
left join trips t on d.id = t.driver_id
``` 
![image](https://user-images.githubusercontent.com/22456230/122685809-5cca5280-d1d3-11eb-8a31-20a978a02647.png)

With a table generated by this query, we can easily calculate trips taken, trip durations, and miles driven for a driver via query or by letting an analysis tool do the work relatively performantly. From here I'll expand this to add the other dimensions, and handle performance gotchas and optimizations as we go. 

[^back](#toc-table-of-contents)

## <a id="toc-a-step-further"></a> A Step Further:

### Adding in everything else
In looking at the initial schema and thinking about insurance rates for people that drive and what I imagine is the most pertinent dimensions to analyze, I'd guess that we're most frequently going to be looking at drivers and their trips (and hence corresponding tables), secondly dicing those metrics to various vehicles, and very infrequently looking at metrics at a profile level. For this reason, I'm going to denormalize the drivers and trips tables into a performant reporting schema and maintain the vehicles and profiles tables in place. Not eliminating joins altogether, but cutting down on what is likely the most frequent one and enabling most users to select from a single table and be far more efficient for users using a drag and drop style analysis tool.

In general, I expect to get a lot of datetime filtering in where clauses, and that is one of the first things I like to deal with. Comparing integers tends to be more performant, especially when we have unknown tools being used that might create auto generated sub queries that inadvertently use a string or something that might get interpreted as a string compared to a datetime which can cause unneeded scans. Columns of integers next to each other will make it faster and less likely to get weird and inefficient optimizer behavior from inhumanly or oddly created queries. So to our initial construct I'll add the additional dimensions and extract some fields yielding:

![image](https://user-images.githubusercontent.com/22456230/122688091-c94b4e80-d1df-11eb-997f-ad90b6238ff6.png)


At this point I'll point out the need to deal with updates to the original fields should, for example, someones birthdate or marital status change. In large sample sizes subtle changes causing data staleness may not have a noticeable impact but in this case I'll also retain the drivers updated_at field as a check constraint that verifies data consistency in an update job that runs at some frequency for completeness and correctness.

At this point, I believe I have a denormalized table encompassing the majority of the fields that will be used. Reducing what is likely the most prominent join and simplifying the primary dataset for analysis. It should be laid out such that a columnar database can filter and calculate on it efficiently despite the queries used to query it. Though some users may need to be encouraged to filter on the integer date part fields since the original datetime values have been left in for completeness.

![image](https://user-images.githubusercontent.com/22456230/122688530-5a232980-d1e2-11eb-82a9-dfeeae1bd250.png)



[^back](#toc-table-of-contents)

## <a id="toc-etl-infrastructure"></a> ETL and Infrastructure:

## <a id="toc-testing-and-debugging"></a> Testing and Debugging:

